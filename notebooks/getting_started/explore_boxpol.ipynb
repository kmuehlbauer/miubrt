{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NetCDF4 output - Explore BoXPol/JuXPol\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**User Info** <br>\n",
    " Global Data - User Input \n",
    " \n",
    " start_time, end_time, location and sweep\n",
    " \n",
    " location = \"BoXPol\" or \"JuXPol\"  (X-band radar)\n",
    " \n",
    " elev_id  = 0 ... 9   (IDs for elevation angles)\n",
    "    \n",
    " For BoXPol, the elevation folders name change in 2017 (check for error message)\n",
    "   \n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import glob\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask\n",
    "import wradlib as wrl\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xpol_path(path=None, start_time=dt.datetime.today(), loc='boxpol'):\n",
    "    \"\"\"Create path of BoXPol/JuXPol radar data files\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    path : str\n",
    "        Path to root folder of radar data, Defaults to None (/auotomount)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    radar_path : str\n",
    "        Path to radar data\n",
    "        \n",
    "    \"\"\"\n",
    "    loc = \"\" if loc.lower()[0:2] == \"bo\" else \"_juelich\"\n",
    "    if path is None:\n",
    "        ins = \"-archiv\" if start_time < dt.datetime(2015,1,1) else \"\" \n",
    "        path = f\"/automount/radar{ins}/scans{loc}\"\n",
    "    date = \"{0}-{1:02d}-{2:02d}\".format(start_time.year, start_time.month, start_time.day)\n",
    "    radar_path = os.path.join(path, \"{0}/{0}-{1:02}/{0}-{1:02d}-{2:02d}\")\n",
    "    return radar_path.format(start_time.year, start_time.month, start_time.day)\n",
    "\n",
    "def get_file_date_regex(filename):\n",
    "    \"\"\"Get regex from filename\n",
    "    \"\"\"\n",
    "    # regex for \"\"%Y-%m-%d--%H:%M:%S\"\n",
    "    reg0 = r\"\\d{4}.\\d{2}.\\d{2}..\\d{2}.\\d{2}.\\d{2}\"\n",
    "    # regex for \"%Y%m%d%H%M%S\" \n",
    "    reg1 = r\"\\d{14}\"\n",
    "    match = re.search(reg0, os.path.basename(filename))\n",
    "    return reg1 if match is None else reg0\n",
    "\n",
    "def get_datetime_from_filename(filename, regex):\n",
    "    \"\"\"Get datetime from filename\n",
    "    \"\"\"\n",
    "    fmt = \"%Y%m%d%H%M%S\"\n",
    "    match = re.search(regex, os.path.basename(filename))\n",
    "    match = \"\".join(re.findall(r\"[0-9]+\", match.group()))\n",
    "    return dt.datetime.strptime(match, fmt)\n",
    "\n",
    "def create_filelist(path_glob, starttime, endtime):\n",
    "    \"\"\"Create filelist from path_glob and filename dates\n",
    "    \"\"\"\n",
    "    file_names = sorted(glob.glob(path_glob))\n",
    "    regex = get_file_date_regex(file_names[0])\n",
    "    for fname in file_names:\n",
    "        time = get_datetime_from_filename(fname, regex)\n",
    "        if time >= starttime and time < endtime:\n",
    "            yield fname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Data - USER INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = dt.datetime(2015, 7, 5, 0, 0)\n",
    "end_time = dt.datetime(2015, 7, 5, 23, 59, 59)\n",
    "location = \"BoXPol\"\n",
    "elev_id  = 0\n",
    "# User Input End #\n",
    "\n",
    "if location == \"BoXPol\":\n",
    "    snames = [\"n_ppi_010deg\",\"ppi_1p5deg\",\"ppi_2p4deg\",\"ppi_3p4deg\",\"n_ppi_045deg\",\"n_ppi_082deg\", \n",
    "          \"n_ppi_110deg\",\"n_ppi_140deg\",\"n_ppi_180deg\",\"n_ppi_280deg\"]\n",
    "    # 2017 onwards, elevations changed ?\n",
    "    #snames = [\"n_ppi_010deg\",\"n_ppi_020deg\",\"n_ppi_031deg\",\"n_ppi_045deg\",\"n_ppi_060deg\",\"n_ppi_082deg\", \n",
    "    #      \"n_ppi_110deg\",\"n_ppi_140deg\", \"n_ppi_180deg\", \"n_ppi_280deg\"]\n",
    "    #\n",
    "    sname  = snames[elev_id]\n",
    "    sweep  = 0\n",
    "elif location == \"JuXPol\":\n",
    "    sname = \"DWD_Vol_2\"\n",
    "    sweep = elev_id\n",
    "\n",
    "output_path = \"\"\n",
    "\n",
    "radar_path = get_xpol_path(start_time=start_time, loc=location)\n",
    "radar_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Netcdf Dataset of wanted radar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_netcdf(ds, fpath, engine='netcdf4'):\n",
    "    \"\"\"Export Dataset to NetCDF file\n",
    "    \"\"\"\n",
    "    delayed_obj = ds.to_netcdf(fpath, engine=engine, compute=False)\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    with ProgressBar():\n",
    "        results = delayed_obj.compute()\n",
    "\n",
    "def save_netcdf_dataset(ds, path, post=None, engine='netcdf4'):\n",
    "    \"\"\"Save Dataset to NetCDF file \n",
    "    \"\"\"\n",
    "    elevation = np.median(ds.elevation)\n",
    "    t0 = ds.time[0].values.astype(\"M8[s]\").astype(\"O\")\n",
    "    t0 = t0.replace(second=0, microsecond=0, minute=((t0.minute // 5) * 5))\n",
    "    t1 = ds.time[-1].values.astype(\"M8[s]\").astype(\"O\") + dt.timedelta(minutes=5)\n",
    "    t1 = t1.replace(second=0, microsecond=0, minute=((t1.minute // 5) * 5))\n",
    "    fname = [f\"{ds.location.values}\", f\"{int(round(elevation, 1)*10)}\", f\"{t0:%Y%m%d}\", f\"{t0:%H%M}\",\n",
    "             f\"{t1:%H%M}\"]\n",
    "    if post is not None:\n",
    "        fname.append(post)\n",
    "    fname = \"_\".join(fname) + \".nc\"\n",
    "    ofname = os.path.join(path, fname)\n",
    "    ds.pipe(to_netcdf, ofname, engine=engine)\n",
    "    return ofname\n",
    "\n",
    "def create_volume(location, name, start_time, end_time, sweep, outpath=\"\"):\n",
    "    \"\"\"Create NetCDF file from radar data\n",
    "    \"\"\"\n",
    "    radar_path = get_xpol_path(start_time=start_time, loc=location)\n",
    "    file_path = os.path.join(radar_path, name)\n",
    "    file_obj = list(create_filelist(os.path.join(file_path, \"*\"), start_time, end_time))\n",
    "    vol = wrl.io.open_odim(file_obj, loader=\"h5py\", flavour=\"GAMIC\", chunks={})\n",
    "    return vol\n",
    "\n",
    "def create_netcdf_dataset(location, name, start_time, end_time, sweep, outpath=\"\", chunks={}, engine='h5netcdf'):\n",
    "    \"\"\"Create NetCDF file from radar data\n",
    "    \"\"\"\n",
    "    radar_path = get_xpol_path(start_time=start_time, loc=location)\n",
    "    file_path = os.path.join(radar_path, name)\n",
    "    file_obj = list(create_filelist(os.path.join(file_path, \"*\"), start_time, end_time))\n",
    "    vol = wrl.io.open_odim(file_obj, loader=\"h5py\", flavour=\"GAMIC\", chunks=chunks)\n",
    "    ds = vol[sweep].data\n",
    "    ds = ds.assign_coords({\"location\": location})\n",
    "    #ds = ds.chunk({\"time\": 24})\n",
    "    ofname = save_netcdf_dataset(ds, outpath, engine=engine)\n",
    "    del vol\n",
    "    return os.path.abspath(ofname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create netcdf Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fname_raw = create_netcdf_dataset(location, sname, start_time, end_time, sweep, outpath=output_path, engine='h5netcdf', chunks=None)\n",
    "fname_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import hvplot\n",
    "import hvplot.xarray\n",
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swp = xr.open_dataset(fname_raw, chunks={\"time\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georeference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swp = swp.pipe(wrl.georef.georeference_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swp = swp.chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbz_plot = swp.hvplot.quadmesh(groupby='time',\n",
    "                               x='x', y='y', \n",
    "                               z='DBZH', \n",
    "                               rasterize=True, \n",
    "                               clim=(-20,50), cmap=mpl.cm.nipy_spectral,\n",
    "                               width=500, #height=300,\n",
    "                               aspect=1,\n",
    "                              )\n",
    "dbz_plot = dbz_plot.options(color_levels=list(np.arange(-20, 51, 2)))\n",
    "dbz_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prom [conda env:prom]",
   "language": "python",
   "name": "conda-env-prom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
